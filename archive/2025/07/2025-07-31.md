# daily-papers

## 2025-07-31


### HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels

[arXiv](https://arxiv.org/abs/2507.21809)

**Authors:** Junta Wu, Zhenwei Wang, HunyuanWorld Team, nightkiller, LeoLau

**Category:** Multi-Modal

**Summary:** HunyuanWorld 1.0 is a novel model designed for generating immersive, explorable, and interactive 3D worlds from various inputs, including text prompts or images. Its primary objective is to overcome the limitations of existing 3D content generation methods by enabling the creation of consistent, high-quality, and interactive 3D environments. The methodology involves a cascaded generation pipeline that integrates a text-to-3D world model, a 3D world-to-mesh model, and a 3D world-to-interaction model, leveraging diffusion models and large language models for scene understanding and generation. Quantitatively, the system achieves a 2.5 times higher user satisfaction score for immersion and interactivity compared to baseline methods, demonstrating its effectiveness in generating coherent and interactive 3D worlds. This advancement provides AI practitioners with a powerful tool for rapidly prototyping and deploying complex virtual environments for applications such as gaming, simulation, and virtual reality.

---

### X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again

[arXiv](https://arxiv.org/abs/2507.22058)

**Authors:** Yongming Rao, Chen Li, Yeyao Ma, Yibing Wang, Zigang Geng

**Category:** Computer Vision

**Summary:** X-Omni significantly enhances discrete autoregressive image generative models (DAIGMs) by integrating reinforcement learning (RL). The core objective is to overcome the limitations of traditional DAIGMs, such as slow sampling and suboptimal image quality, by framing image generation as a sequential decision-making process. The paper introduces X-Omni, an RL-based framework that employs a Transformer-based policy network and a learned reward function to optimize image generation. Notably, X-Omni achieves a new state-of-the-art FID score of 2.12 on ImageNet 256x256, demonstrating superior performance and faster sampling speeds compared to existing DAIGMs. This implies that AI practitioners can now leverage RL to develop more efficient and higher-quality discrete autoregressive image generation systems, opening new avenues for controllable image synthesis and data augmentation.

---

### ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical Knowledge

[arXiv](https://arxiv.org/abs/2507.21990)

**Authors:** Xuanze Lin, Lu Chen, Ziping Wan, Bo Chen, Zihan Zhao

**Category:** Natural Language Processing

**Summary:** The paper introduces ChemDFM-R, a novel chemical reasoner large language model (LLM) designed to enhance chemical understanding and reasoning. Its primary objective is to address the limitations of current LLMs in chemical domain tasks by integrating atomized chemical knowledge through fine-tuning. The methodology involves a two-stage fine-tuning process: first, using a large dataset of chemical questions, and second, employing supervised fine-tuning with specifically curated chemical expert demonstrations. ChemDFM-R achieved a significant improvement, outperforming GPT-4 by 11.2% on the ChemDFM-R benchmark for chemical reasoning. The main implication for AI practitioners is the potential for more accurate and reliable AI systems in chemistry, reducing the need for extensive manual data annotation and enhancing drug discovery and material science applications.

---

### CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning

[arXiv](https://arxiv.org/abs/2507.14111)

**Authors:** Chris Shum, Jiwei Li, Albert Wang, Xiaofei Sun, xxiaoyali

**Category:** Reinforcement Learning

**Summary:** This paper introduces CUDA-L1, a novel approach for optimizing CUDA kernels through contrastive reinforcement learning. The core objective is to automate and enhance the efficiency of CUDA kernel optimizations, traditionally a manual and complex process. CUDA-L1 employs a contrastive learning framework within a reinforcement learning setup, where a policy network learns to select optimal optimization sequences by comparing positive (better-performing) and negative (worse-performing) states. The method demonstrates significant performance improvements, achieving an average speedup of 1.40x across various benchmarks compared to expert-optimized kernels. This work implies that AI practitioners can leverage reinforcement learning to achieve substantial, automated performance gains in specialized hardware programming, reducing the need for extensive manual tuning.

---

### MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge

[arXiv](https://arxiv.org/abs/2507.21183)

**Authors:** Daoan Zhang, Tianle Wang, Sipeng Zhang, YWZBrandon, Eric-Lan

**Category:** Reinforcement Learning

**Summary:** MaPPO introduces a novel approach for aligning large language models with human preferences by integrating prior knowledge directly into the preference optimization process. The core objective is to improve the efficiency and stability of preference-based reinforcement learning from human feedback. This is achieved by formulating a maximum a posteriori objective that explicitly incorporates prior distributions over learned policies, thereby regularizing the optimization. Empirical results demonstrate that MaPPO outperforms existing methods like DPO and PPO-ptx on various benchmarks, achieving an average win rate improvement of 5% over DPO on the TL;DR dataset, and significantly reducing policy collapse. This methodology offers a more robust and data-efficient way to train preference models, leading to better-behaved and more aligned AI systems for practitioners.

---

### AnimalClue: Recognizing Animals by their Traces

[arXiv](https://arxiv.org/abs/2507.20240)

**Authors:** Hirokatsu Kataoka, Christian Rupprecht, Iro Laina, Nakamasa Inoue, Risa Shinoda

**Category:** Computer Vision

**Summary:** The paper "AnimalClue: Recognizing Animals by their Traces" introduces a novel approach for animal recognition using only trace evidence like paw prints, fur, or scat. Its primary objective is to develop a robust system that can accurately identify animal species from these subtle environmental clues, a task often challenging for humans. The methodology involves a convolutional neural network architecture trained on a new dataset of trace images, employing advanced image processing techniques to enhance feature extraction from ambiguous visual data. The system achieved a 92.5% accuracy rate in classifying animal species from trace images, significantly outperforming traditional recognition methods. This research implies that AI practitioners can develop highly specialized computer vision systems for niche environmental monitoring and ecological study applications, even with limited visual information.

---

### MOVE: Motion-Guided Few-Shot Video Object Segmentation

[arXiv](https://arxiv.org/abs/2507.22061)

**Authors:** Henghui Ding, Hengrui Hu, Kaining Ying

**Category:** Computer Vision

**Summary:** The paper "MOVE: Motion-Guided Few-Shot Video Object Segmentation" introduces a novel approach for few-shot video object segmentation (FSVOS) by leveraging motion information. The main objective is to overcome the limitations of current FSVOS methods that struggle with significant object and background changes, by incorporating motion cues to improve segmentation accuracy. The key methodology involves a motion-guided framework that utilizes a temporal affinity-based motion encoder and a motion-guided mask decoder, enabling effective propagation of object information across frames. Experimental results demonstrate that MOVE achieves state-of-the-art performance, with a J&F score of 72.8 on the YouTube-VOS dataset, outperforming previous methods by a notable margin. This implies that AI practitioners can achieve more robust and accurate video object segmentation in real-world applications by integrating motion information, particularly in scenarios with dynamic scenes and limited annotated data.

---

### MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions

[arXiv](https://arxiv.org/abs/2507.21503)

**Authors:** Peng Zhang, Jitao Sang, Xiangxu Zhang, Shitong Duan, yanxuzhu

**Category:** Multi-Modal

**Summary:** This paper introduces MoHoBench, a benchmark designed to evaluate the honesty of Multimodal Large Language Models (MLLMs) by posing unanswerable visual questions. The core objective is to assess whether MLLMs truthfully admit they cannot answer questions unsupported by the provided image, rather than hallucinating responses. MoHoBench creates over 20,000 unanswerable questions by modifying existing Visual Question Answering (VQA) datasets to introduce false premises, and evaluates models based on their ability to generate "I don't know" responses. Experimental results show that current MLLMs exhibit poor honesty, with a mean honesty score of only 11.0 on MoHoBench, indicating a significant tendency to hallucinate. This work highlights a critical area for improvement in MLLM robustness, suggesting that developers should prioritize training models to recognize and abstain from answering unanswerable queries to enhance their trustworthiness.

---

### Evaluating Deep Learning Models for African Wildlife Image Classification: From DenseNet to Vision Transformers

[arXiv](https://arxiv.org/abs/2507.21364)

**Authors:** Almustapha A Wakili, Nasiru Muhammad, Bilqisu Ismail, Umar Sani Muhammad, lukmanaj

**Category:** Computer Vision

**Summary:** This paper evaluates the efficacy of deep learning models for classifying African wildlife in imagery, specifically addressing the challenge of limited and imbalanced datasets from conservation efforts. The main objective was to compare the performance of various convolutional neural networks (DenseNet, ResNet, VGG, Inception) against Vision Transformers (ViT) for this specialized task. The methodology involved fine-tuning pre-trained models and employing techniques like data augmentation and transfer learning, while also exploring the impact of data imbalance. Key results indicate that DenseNet201 achieved the highest accuracy at 98.6%, outperforming ViT, particularly on smaller datasets. The main implication for AI practitioners is that for wildlife image classification with limited data, well-established CNN architectures like DenseNet may still be more effective than newer transformer models, suggesting the importance of model choice based on data characteristics.

---
