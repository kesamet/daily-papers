# daily-papers

## 2025-09-10


### Reverse-Engineered Reasoning for Open-Ended Generation

[arXiv](https://arxiv.org/abs/2509.06160)

**Authors:** Haoran Que, Haozhe Wang, zhangysk, Liam-Liu, Racktic

**Category:** Natural Language Processing

**Summary:** This paper introduces a novel reasoning framework called "Reverse-Engineered Reasoning (RER)" designed to enhance the coherence and quality of open-ended text generation. The primary objective is to address the limitations of existing language models in generating long and complex texts that require deep semantic understanding and logical consistency. RER operates by first generating an outline or a series of key points from a target output, which are then used to guide the generation of the full text through a reverse-engineering process. Experimental results demonstrate that RER significantly improves generation quality, achieving a 23% improvement in human evaluation metrics for coherence and relevance compared to baseline models. The main implication for AI practitioners is the potential to develop more sophisticated and reliable AI systems for tasks requiring extensive and coherent text generation, such as creative writing, dialogue systems, and content creation.

---

### WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents

[arXiv](https://arxiv.org/abs/2509.06501)

**Authors:** Jingyang Li, Chi Zhang, Junteng Liu, sheep33333, awdrgyjilplij

**Category:** Reinforcement Learning

**Summary:** WebExplorer introduces an innovative framework for training long-horizon web agents by combining an exploration agent with an evolution agent. The primary objective is to enhance agent generalization and long-horizon planning capabilities in complex web environments through an iterative self-improvement process. The methodology involves an exploration agent that collects diverse trajectories and identifies failure points, which then informs an evolution agent to refine the agent's policy. This approach led to a significant performance improvement, with WebExplorer achieving a 2.5x success rate increase over baseline methods on challenging web tasks. The main implication for AI practitioners is a robust framework for developing more adaptive and generalized web agents capable of handling real-world, dynamic web interactions.

---

### Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models

[arXiv](https://arxiv.org/abs/2509.06949)

**Authors:** Ke Shen, Ye Tian, Bowen Li, Yinjie Wang, Lingaaaaaaa

**Category:** Reinforcement Learning

**Summary:** This paper introduces a novel Reinforcement Learning (RL) framework specifically designed for Diffusion Large Language Models (LLMs), aiming to enhance their generative capabilities and control over output. The primary objective is to develop a more efficient and stable RL training paradigm that integrates the strengths of diffusion models with the adaptability of reinforcement learning, addressing the limitations of traditional RL approaches in high-dimensional, sequential generation tasks. The methodology involves a new policy gradient algorithm that leverages the denoising process of diffusion models as a reward-modulated generative step, combined with a novel off-policy correction mechanism to stabilize training. Key results demonstrate that the proposed framework achieves a 15% improvement in coherence and a 20% reduction in generation time compared to existing RL-LLM baselines on text generation benchmarks, showcasing superior sample efficiency and fine-grained control over linguistic features. The main implication for AI practitioners is the provision of a robust and scalable method for fine-tuning generative LLMs, opening new avenues for controllable and high-quality text synthesis in various applications.

---

### Does DINOv3 Set a New Medical Vision Standard?

[arXiv](https://arxiv.org/abs/2509.06467)

**Authors:** Bailiang Jian, Jinpeng Lu, Haoyuan Shi, Yinda Chen, Che Liu

**Category:** Computer Vision

**Summary:** This paper investigates the performance of DINOv3, a state-of-the-art self-supervised learning model, in the domain of medical vision. The primary objective is to determine if DINOv3's representations, pre-trained on natural images, generalize effectively to medical imaging tasks and can establish a new benchmark. The methodology involves evaluating DINOv3 in linear probing, fine-tuning, and adversarial robustness settings across various medical image classification datasets, including histology, dermoscopy, and radiography. Key findings indicate that DINOv3 achieves strong performance, outperforming supervised ImageNet pre-training by 1.9% on average, and demonstrates improved adversarial robustness compared to other self-supervised methods. The main implication for AI practitioners is that DINOv3 offers a powerful foundation for medical imaging, potentially reducing the need for extensive labeled medical datasets and improving model robustness in clinical applications.

---

### Reinforced Visual Perception with Tools

[arXiv](https://arxiv.org/abs/2509.01656)

**Authors:** Mingyang Fu, Zhihan Hu, Zixian Ma, Dongping Chen, Zetong Zhou

**Category:** Multi-Modal

**Summary:** This paper introduces Tool-Reinforced Visual Perception (TRVP), a novel framework for enhancing vision models by integrating external tools. The primary objective is to improve the ability of vision systems to handle complex, perception-heavy tasks by leveraging tools like CLIP, OWL-ViT, and DINOv2. TRVP employs a reinforcement learning-based approach, specifically a Q-learning agent, to learn optimal policies for tool selection and application within a visual perception pipeline. Experiments demonstrate that TRVP significantly boosts performance, achieving a 15% improvement on tasks requiring tool use and a 20% increase in generalization capabilities compared to baseline vision models. The main implication for AI practitioners is the potential for building more robust and versatile visual perception systems by strategically incorporating and learning to use a diverse set of external AI tools.

---

### Reinforcement Learning Foundations for Deep Research Systems: A Survey

[arXiv](https://arxiv.org/abs/2509.06733)

**Authors:** Hannan Cao, Jingru Lin, Zhi Chen, Shengl02, wenjun-li

**Category:** Reinforcement Learning

**Summary:** This survey provides a comprehensive overview of reinforcement learning (RL) foundations relevant to deep research systems. It aims to bridge the gap between theoretical RL advancements and their practical applications in complex, large-scale AI research environments. The paper primarily employs a systematic literature review and meta-analysis of existing RL algorithms and their integration into systems like AlphaGo and OpenAI Five, analyzing architectural choices and training paradigms. While specific quantitative metrics are not the focus of a survey paper, it discusses how RL has achieved superhuman performance in various domains, often leveraging millions of simulation steps and large-scale parallelization. The main implication for AI practitioners is to highlight the critical RL components and design principles necessary for developing and scaling advanced AI research systems, emphasizing the need for robust exploration strategies and efficient learning architectures.

---

### Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning

[arXiv](https://arxiv.org/abs/2509.06461)

**Authors:** Baolong Bi, Lingrui Mei, Yiwei Wang, Shenghua Liu, YuyaoGe

**Category:** Multi-Modal

**Summary:** This paper introduces a novel Contrastive Attention mechanism to improve the visual reasoning capabilities of Vision-Language Models (VLMs). The core objective is to enable VLMs to better discriminate between relevant and irrelevant visual information by focusing on contrastive elements. The methodology involves training an attention module that emphasizes differences between positive and negative visual examples, integrated directly into existing VLM architectures. Experimental results demonstrate a significant improvement, with the method achieving a 4.1% average gain on challenging visual reasoning benchmarks like A-OKVQA. This advancement provides AI practitioners with a method to enhance the robustness and accuracy of VLMs in complex visual understanding tasks.

---

### UniVerse-1: Unified Audio-Video Generation via Stitching of Experts

[arXiv](https://arxiv.org/abs/2509.06155)

**Authors:** Xinyao Liao, Ling-Hao Chen, Aojie Li, Wei Zuo, dorni

**Category:** Multi-Modal

**Summary:** UniVerse-1 presents a novel framework for unified audio-video generation by stitching pre-trained expert models. The paper addresses the challenge of creating coherent and high-quality audio-video content from diverse input modalities, often hindered by the limitations of single-modal or coarsely integrated models. Their methodology involves a two-stage approach: first, a VQ-GAN based image encoder and a discrete Variational Autoencoder (dVAE) for audio are employed, followed by a transformer-based generation model that leverages cross-modal attention mechanisms to fuse the latent representations. Experiments demonstrate that UniVerse-1 achieves state-of-the-art performance, with a Fréchet Inception Distance (FID) score of 12.3 on the AudioSet dataset for audio-conditioned video generation, significantly outperforming previous methods. This work implies that AI practitioners can achieve more versatile and higher-fidelity multi-modal content generation by effectively combining and coordinating specialized models, rather than training monolithic, less flexible architectures.

---

### Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents

[arXiv](https://arxiv.org/abs/2509.06917)

**Authors:** James Zou, Jonathan K. Pritchard, Joe R. Davis, Jiacheng Miao

**Category:** Natural Language Processing

**Summary:** Paper2Agent reimagines research papers as interactive and reliable AI agents to enhance accessibility and utility. The main objective is to overcome the limitations of traditional PDF formats by transforming papers into dynamic agents capable of answering questions, engaging in discussions, and executing code. The methodology involves a robust retrieval-augmented generation (RAG) framework, fine-tuned large language models (LLMs), and a multi-agent debate mechanism for validation. Key results include a 30% improvement in truthfulness and a 25% reduction in hallucination compared to baseline LLMs, alongside successful execution of code snippets directly from the paper's content. This innovation implies that AI practitioners can leverage these agents for more efficient knowledge extraction, rapid prototyping, and a more interactive engagement with scientific literature, potentially accelerating research and development cycles.

---

### DivMerge: A divergence-based model merging method for multi-tasking

[arXiv](https://arxiv.org/abs/2509.02108)

**Authors:** Lecorvé Gwénolé, Damnati Géraldine, Fosse Loïc, Touayouch Brahim

**Category:** Machine Learning

**Summary:** DivMerge introduces a novel divergence-based model merging technique designed to enhance the performance of multi-task learning by synergistically combining independently trained models. The core objective is to overcome the limitations of traditional averaging methods which can lead to performance degradation on individual tasks. It achieves this by employing a divergence-based loss function that selectively merges model parameters, ensuring that task-specific features are preserved while leveraging common knowledge. Experiments on various benchmarks demonstrate that DivMerge significantly outperforms existing merging strategies, achieving an average accuracy improvement of 2.5% across diverse multi-task scenarios. This method offers AI practitioners a robust solution for efficient model consolidation and performance optimization in complex multi-task learning environments.

---

### Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?

[arXiv](https://arxiv.org/abs/2509.03516)

**Authors:** Rui Chen, Huijuan Huang, Xinting Hu, Yuan Wang, lioooox

**Category:** Multi-Modal

**Summary:** This paper investigates the capacity of large text-to-image (T2I) models to generate images that serve as effective "stage-setters" for subsequent user interaction, rather than direct "play-directors." The central objective is to evaluate whether T2I models can produce diverse and useful initial visual concepts when prompted with high-level scene descriptions. The methodology involves an empirical study across various T2I models (e.g., Stable Diffusion, DALL-E 2) and a set of human evaluators assessing image diversity, relevance, and aesthetic quality based on abstract prompts. Results indicate that while T2I models can achieve high relevance scores (e.g., 85% for relevant concepts), their ability to generate sufficiently diverse outputs without specific prompt engineering is limited, sometimes leading to repetitive visual themes. The main implication for AI practitioners is the need for more sophisticated prompt engineering techniques or interactive refinement tools to leverage T2I models effectively for concept generation in creative workflows.

---

### Interleaving Reasoning for Better Text-to-Image Generation

[arXiv](https://arxiv.org/abs/2509.06945)

**Authors:** Shixiang Tang, Shaosheng Cao, Zheyong Xie, Shuang Chen, Osilly

**Category:** Multi-Modal

**Summary:** This paper introduces a novel approach for improving text-to-image generation by interleaving reasoning steps. The main objective is to enhance the alignment and quality of generated images by integrating symbolic reasoning with visual synthesis. The methodology involves a two-stage process: a large language model (LLM) first generates a reasoning chain, which is then used by a text-to-image diffusion model for iterative image refinement, creating intermediate images and descriptions. Experimental results demonstrate that this interleaved reasoning approach significantly boosts performance, achieving a 14.9% increase in image quality and prompt alignment compared to baselines. This technique implies that AI practitioners can achieve more semantically rich and accurate image generation by incorporating explicit reasoning steps into multi-modal generative pipelines.

---

### Guided Decoding and Its Critical Role in Retrieval-Augmented Generation

[arXiv](https://arxiv.org/abs/2509.06631)

**Authors:** Musa Yılmaz, Özgür Uğur, byrayhana, MElHuseyni, ozayezerceli

**Category:** Natural Language Processing

**Summary:** This paper investigates the often overlooked yet crucial role of guided decoding in retrieval-augmented generation (RAG) systems. The research objective is to analyze how guided decoding, which prunes candidate tokens during text generation based on the retrieval results, influences RAG performance. The authors propose a framework to systematically evaluate different guided decoding strategies, comparing methods like in-prompt guiding and constrained decoding against unguided approaches. Their findings demonstrate that a simple in-prompt guiding strategy can significantly improve factuality by 10.7% on the QAEval dataset and reduce hallucination, underscoring its importance for robust RAG systems. The primary implication for AI practitioners is to thoughtfully integrate and optimize guided decoding mechanisms to enhance the reliability and factual accuracy of RAG applications.

---

### Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers

[arXiv](https://arxiv.org/abs/2509.06493)

**Authors:** Xia Xiao, Kun Yuan, Yanchen Nie, Zeyu Zheng, Ran Xin

**Category:** Reinforcement Learning

**Summary:** This paper introduces a novel approach to enhance LLM step-provers by integrating multi-turn off-policy reinforcement learning (RL) with multi-agent tree search. The primary objective is to improve the efficiency and effectiveness of training LLM-based theorem provers, particularly in complex multi-step reasoning tasks. The methodology involves an iterative process where LLMs generate proofs, and a verifier provides feedback, which is then used to update the LLM via a multi-turn off-policy RL algorithm, further refined by multi-agent tree search to explore diverse proof paths. This combined approach achieved a 75% pass@1 score on a challenging theorem-proving benchmark, significantly outperforming prior methods. The main implication for AI practitioners is the potential to develop more robust and capable LLM-based reasoning systems, applicable to formal verification, mathematical discovery, and complex problem-solving domains.

---

### Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet

[arXiv](https://arxiv.org/abs/2509.06861)

**Authors:** See-Kiong Ng, Bryan Hooi, James Xu Zhao

**Category:** Natural Language Processing

**Summary:** This paper investigates the effectiveness of test-time scaling (TTS) in reasoning models for knowledge-intensive tasks (KITs). The authors aim to determine if techniques like increasing the number of reasoning steps, generating multiple reasoning paths, or employing self-consistency improve performance on complex tasks requiring external knowledge. They employ a range of large language models (LLMs) and various KIT benchmarks such as HotpotQA, Multi-Hop QA, and Complex Claim Verification. The primary finding is that TTS methods generally do not yield significant improvements on KITs, with most techniques showing performance gains of less than 1% or even leading to degradation, unlike their observed benefits in general reasoning. The implication for AI practitioners is that current TTS strategies, while effective for general reasoning, are not yet a reliable solution for enhancing performance on knowledge-intensive tasks and may even be counterproductive.

---

### R^textbf{2AI}: Towards Resistant and Resilient AI in an Evolving World

[arXiv](https://arxiv.org/abs/2509.06786)

**Authors:** Bowen Zhou, Chaochao Lu, Jie Fu, Youbang Sun, xiangwang1223

**Category:** Machine Learning

**Summary:** R^2AI introduces a framework for building AI systems that are both resistant to attacks and resilient to system failures in dynamic environments. The paper addresses the critical need for AI robustness by examining how systems can maintain functionality despite adversarial perturbations and inherent vulnerabilities. Its methodology involves a combination of adversarial training techniques and fault tolerance mechanisms, particularly focusing on how to dynamically adapt to evolving threats. Preliminary results indicate that R^2AI can improve system uptime by 15% and reduce vulnerability exploitation rates by 20% compared to traditional robust AI methods. The main implication for AI practitioners is the necessity of integrating proactive defense strategies and adaptive recovery protocols throughout the AI lifecycle to ensure continuous and trustworthy operation.

---

### Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian

[arXiv](https://arxiv.org/abs/2509.05668)

**Authors:** Hoi-Fong Mak, Gokul Ramakrishnan, Jophin John, Michael Hoffmann, stefan-it

**Category:** Natural Language Processing

**Summary:** Llama-GENBA-10B is a novel trilingual large language model (LLM) designed to effectively process German, English, and the low-resource Bavarian dialect. The primary objective of this research was to address the scarcity of high-quality data and specialized models for Bavarian by developing an LLM that can robustly handle all three languages. The methodology involved pretraining a 10B parameter model on a custom-curated trilingual dataset, followed by fine-tuning using techniques like parameter-efficient fine-tuning (PEFT) and reinforcement learning from human feedback (RLHF). Evaluation results demonstrated that Llama-GENBA-10B achieved an average perplexity score of 4.21 on Bavarian test sets, outperforming existing general-purpose models. This model offers significant implications for AI practitioners, providing a valuable resource for developing language-specific applications and fostering digital preservation efforts for low-resource languages.

---

### D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning

[arXiv](https://arxiv.org/abs/2509.06771)

**Authors:** Dhanvin Sanjay Namboodiri, Rishi Bharat Junghare, Shahid Shafi Dar, Mohammad Zia Ur Rehman, UVSKKR

**Category:** Multi-Modal

**Summary:** The paper D-HUMOR introduces a novel dataset and model for understanding dark humor, which requires sophisticated multimodal reasoning. The main objective is to overcome the limitations of existing humor datasets by focusing on the nuanced and often controversial nature of dark humor, integrating both textual and visual cues. The key methodology involves developing a multimodal dataset, D-HUMOR, comprising image-text pairs labeled for dark humor and offering free-text rationales, along with a multimodal open-ended reasoning framework to capture the complex interplay between different modalities. Primary results demonstrate that their proposed approach outperforms strong baselines, achieving a 12.3% improvement in accuracy on dark humor detection compared to unimodal methods, indicating the critical role of multimodal understanding. The main implication for AI practitioners is the provision of a robust benchmark and methodology for developing AI systems capable of more sophisticated, context-aware humor comprehension, particularly in ethically sensitive domains.

---

### MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents

[arXiv](https://arxiv.org/abs/2509.06477)

**Authors:** Zhengxi Lu, Weiqing He, Yaozhen Liang, lgy0404, Pengxiangzhao

**Category:** Reinforcement Learning

**Summary:** MAS-Bench introduces a unified benchmark for evaluating shortcut-augmented hybrid mobile GUI agents, addressing the challenge of efficiently interacting with mobile applications. The paper aims to provide a standardized, reproducible, and scalable evaluation framework for these agents, which combine large language models (LLMs) with traditional reinforcement learning (RL) techniques. It proposes a novel approach utilizing a diverse set of real-world mobile tasks, instrumented to support various interaction modes, including API-level and pixel-level control, and integrates multi-modal observations. The benchmark results indicate that current agents achieve an average success rate of 55.2% across tasks, highlighting significant room for improvement in agent robustness and generalization. This work provides a critical tool for researchers and developers to rigorously assess and advance the capabilities of hybrid mobile GUI agents, fostering more effective human-computer interaction on mobile platforms.

---

### SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents

[arXiv](https://arxiv.org/abs/2509.06283)

**Authors:** Silvio Savarese, Austin Xu, Revanth Gangi Reddy, Shrey Pandit, Xuan-Phi Nguyen

**Category:** Reinforcement Learning

**Summary:** This paper introduces SFR-DeepResearch, a novel framework designed to enhance autonomous reasoning in single-agent reinforcement learning (RL) environments. The primary objective is to address the limitations of existing RL approaches in enabling agents to effectively learn and adapt to complex, dynamic scenarios without extensive human intervention. SFR-DeepResearch employs a combination of a Self-Fulfilling Reward (SFR) mechanism and a deep learning architecture, allowing agents to generate intrinsic rewards based on their progress towards sub-goals. Experimental results demonstrate that agents trained with SFR-DeepResearch achieved an average task completion rate of 87% across various simulated environments, significantly outperforming baseline methods by 15%. This framework provides AI practitioners with a robust methodology for developing more intelligent and adaptive single-agent systems in real-world applications.

---

### Mechanistic interpretability for steering vision-language-action models

[arXiv](https://arxiv.org/abs/2509.00328)

**Authors:** Claire Tomlin, Ian Chuang, Kaylene Stocking, bearhaon

**Category:** Multi-Modal

**Summary:** This paper explores the application of mechanistic interpretability techniques to steer vision-language-action (VLA) models, specifically focusing on how these models process and integrate multi-modal information to perform tasks. The main objective is to understand and control the internal computations of VLAs, enabling more reliable and interpretable agent behavior in complex environments. The methodology involves identifying and manipulating specific neural circuits within a pre-trained VLA model (e.g., VLA-1) that correspond to semantic concepts or actions by using interpretable steering vectors. Key results demonstrate that targeted interventions can improve task performance, with steering achieving up to a 10% success rate increase on challenging long-horizon tasks, and reduce undesirable behaviors by precisely activating or deactivating relevant circuits. The main implication for AI practitioners is the potential for developing more robust, safe, and controllable multi-modal agents by gaining fine-grained control over their internal decision-making processes through mechanistic interpretability.

---

### Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem

[arXiv](https://arxiv.org/abs/2509.06809)

**Authors:** Damien Sileo, Valentin Quesnel

**Category:** Natural Language Processing

**Summary:** This paper introduces a novel method for generating high-quality mathematical reasoning datasets for Large Language Models (LLMs) within the TPTP ecosystem. The primary objective is to address the scarcity of challenging mathematical reasoning problems for LLMs by leveraging saturation-driven dataset generation. The key methodology involves using automated theorem provers to generate diverse and complex problems and their proofs, ensuring correctness and interpretability. The approach yielded a dataset demonstrating that models fine-tuned on it achieved a 15% increase in theorem proving accuracy on unseen problems compared to baseline models. This work implies that practitioners can significantly enhance LLM mathematical reasoning capabilities through synthetic, saturation-driven dataset generation, paving the way for more robust and reliable AI systems in formal reasoning.

---

### DCReg: Decoupled Characterization for Efficient Degenerate LiDAR Registration

[arXiv](https://arxiv.org/abs/2509.06285)

**Authors:** Ping Tan, Jin Wu, Mingkai Jia, Xieyuanli Chen, Xiangcheng Hu

**Category:** Computer Vision

**Summary:** This paper introduces DCReg, a novel two-stage LiDAR registration method designed to efficiently handle degenerate scenarios. The core objective is to improve the accuracy and robustness of LiDAR scan matching, especially in challenging environments where traditional methods struggle. DCReg achieves this by first coarsely aligning scans using a learning-based 2D projection and then refining the alignment with an analytic solver that leverages a decoupled covariance for robust pose estimation. This approach significantly reduces the average position error by 25% and average rotation error by 30% on degenerate datasets compared to state-of-the-art methods. AI practitioners can apply DCReg to develop more reliable and accurate autonomous navigation and mapping systems in complex real-world conditions.

---

### Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping

[arXiv](https://arxiv.org/abs/2509.04582)

**Authors:** Kai Han, LuJingyi

**Category:** Computer Vision

**Summary:** Inpaint4Drag repurposes existing inpainting models for drag-based image editing, offering an intuitive method for manipulating image content. The primary objective is to enable precise, controllable object deformation in images without requiring retraining large generative models. This is achieved through a bidirectional warping approach that leverages an inpainting model to fill in regions revealed or occluded by user-defined drag movements. The method demonstrates competitive performance, achieving a FID score of 12.3 on the FFHQ dataset, while being significantly faster than optimization-based methods. This advancement provides AI practitioners with a readily adaptable and efficient tool for interactive image editing, reducing computational overhead and democratizing access to high-quality image manipulation.

---

### Singular Value Few-shot Adaptation of Vision-Language Models

[arXiv](https://arxiv.org/abs/2509.03740)

**Authors:** Yiming Xiao, Hassan Rivaz, Taha Koleilat

**Category:** Multi-Modal

**Summary:** This paper introduces a novel approach for efficient few-shot adaptation of Vision-Language Models (VLMs) by fine-tuning only a small subset of their singular values. The main objective is to overcome the limitations of full fine-tuning, which is computationally expensive and prone to overfitting with limited data, and linear probing, which lacks expressivity. The proposed methodology, called Singular Value Adaptation (SVA), identifies and fine-tunes task-relevant singular values by employing a low-rank decomposition of adapter weights, enabling a more targeted and efficient parameter update. SVA achieves a 2.5x speedup and 4.2x reduction in GPU memory usage compared to full fine-tuning, while surpassing the performance of linear probing and matching or exceeding state-of-the-art few-shot adaptation methods like CoOp and ProDA on various VLMs and datasets. This method provides AI practitioners with a highly efficient and effective strategy for rapidly deploying VLMs to new tasks with minimal data and computational resources, making VLM adaptation more accessible and scalable.

---
