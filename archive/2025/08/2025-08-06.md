# daily-papers

## 2025-08-06


### Qwen-Image Technical Report

[arXiv](https://arxiv.org/abs/2508.02324)

**Authors:** Kaiyuan Gao, Junyang Lin, Jingren Zhou, Jiahao Li, Chenfei Wu

**Category:** Multi-Modal

**Summary:** Qwen-Image is a high-performance multi-modal large language model developed to handle a variety of vision-language tasks by integrating a visual encoder with the Qwen-7B LLM. The research aims to explore the capabilities of large language models in multi-modal understanding and generation tasks. It employs a two-stage training strategy: an initial pre-training phase for visual-text alignment followed by a fine-tuning phase on multi-modal instruction data. Qwen-Image achieves strong performance, outperforming existing models like MiniGPT-4 and LLaVA across 11 benchmarks, with notable improvements in image captioning and visual question answering, evidenced by a CIDEr score of 127.8 on COCO test. The model provides a robust foundation for multi-modal AI applications, indicating a significant step towards more generalized and capable vision-language understanding systems.

---

### SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension

[arXiv](https://arxiv.org/abs/2508.01959)

**Authors:** Liyan Xu, Lemao Liu, Yuqing Li, Jiangnan Li, Junjie Wu

**Category:** Natural Language Processing

**Summary:** SitEmb-v1.5 introduces a significant advancement in dense retrieval models, specifically targeting improved context-aware understanding for semantic association and long story comprehension. The paper aims to overcome the limitations of traditional dense retrievers in capturing nuanced contextual information over extended text sequences. The key methodology involves training a retrieve-and-reread model on a novel dataset comprising a diverse set of synthetic long stories and human-curated shorter texts, alongside the use of Negative Mining 2.0 and specialized loss functions. Results show that SitEmb-v1.5 achieves a 6.2% improvement in performance on the LoCo benchmark, indicating its superior capability in handling complex contextual queries. This work implies that AI practitioners can leverage SitEmb-v1.5 for more accurate and contextually rich information retrieval in applications requiring deep understanding of long-form content.

---

### CellForge: Agentic Design of Virtual Cell Models

[arXiv](https://arxiv.org/abs/2508.02276)

**Authors:** Daniel Shao, Yan Cui, Jiapeng Chen, Zhuoyun Yu, Xiangru Tang

**Category:** Other

**Summary:** CellForge introduces an agentic framework for designing and simulating virtual cell models, addressing the challenge of creating complex biological systems from high-level specifications. The framework utilizes a large language model (LLM) to translate natural language descriptions into a modular, executable simulation, bridging the gap between biological intuition and computational implementation. Key methodologies include an iterative refinement loop where the LLM interacts with a simulator to validate and optimize the cell model, guided by biological principles and user feedback. The system successfully generates functional cell models, demonstrating the ability to accurately reproduce target behaviors with, for instance, a 90% success rate in achieving specified cellular interactions. This approach implies that AI practitioners can leverage agentic LLMs to accelerate scientific discovery and engineering in complex biological or similar multi-component systems, reducing manual programming effort and facilitating interdisciplinary research.

---

### Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following

[arXiv](https://arxiv.org/abs/2508.02150)

**Authors:** Jiaqing Liang, Jie Zeng, Bowei Zhang, Qianyu He, Qingyu Ren

**Category:** Reinforcement Learning

**Summary:** This paper introduces a novel self-supervised reinforcement learning (RL) framework designed to enhance instruction following in reasoning models without requiring extensive human annotations. The core objective is to overcome the trade-off between instruction following and reasoning abilities, especially when models are fine-tuned with instruction-following datasets. The methodology involves an iterative process where a reasoning model acts as the environment, generating diverse reasoning paths which are then optimized through RL to improve adherence to instructions. Empirical results demonstrate that the proposed method significantly improves instruction following by 13.9% on the GSM8K dataset while maintaining reasoning capabilities. This framework offers AI practitioners a scalable and efficient way to develop more robust and controllable reasoning models, mitigating the need for costly human-labeled data in complex reasoning tasks.

---

### Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report

[arXiv](https://arxiv.org/abs/2508.01059)

**Authors:** Anu Vellore, Baturay Saglam, Blaine Nelson, Paul Kassianik, Sajana Weerawardhena

**Category:** Natural Language Processing

**Summary:** The Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report focuses on developing and evaluating an 8B-parameter instruction-tuned large language model (LLM) specifically designed for cybersecurity applications. The primary objective was to create a robust and safe LLM that can assist in identifying and mitigating security vulnerabilities. The methodology involved fine-tuning the base Llama-3.1-8B model on a curated dataset of cybersecurity-related instructions and responses, emphasizing safety and helpfulness. Key results indicate that the Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct achieves a 95.2% helpfulness score on cybersecurity tasks while maintaining a low refusal rate for unsafe queries. This model offers a significant advancement for AI practitioners in developing specialized and secure AI agents for various cybersecurity operations, from threat detection to vulnerability assessment.

---

### VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo

[arXiv](https://arxiv.org/abs/2508.02317)

**Authors:** Bin Jia, Zhongkai Zhao, Zhelun Shi, Yaowei Zheng, Qianli Ma

**Category:** Multi-Modal

**Summary:** VeOmni introduces a novel model-centric distributed recipe zoo designed to scale the training of diverse large-scale multi-modal models efficiently. The core objective is to reduce the complexity and cost associated with optimizing distributed training for various architectures and data types by providing pre-validated, adaptable recipes. The methodology involves a framework that integrates model, parallelism, and optimization strategies, facilitating automatic recipe generation and fine-tuning across different modalities. VeOmni demonstrates significant training speedups, achieving up to 3.7x faster training compared to existing methods on certain models, while maintaining model performance. This approach enables AI practitioners to accelerate the development and deployment of new multi-modal AI systems by simplifying the distributed training pipeline.

---

### Fitness aligned structural modeling enables scalable virtual screening with AuroBind

[arXiv](https://arxiv.org/abs/2508.02137)

**Authors:** Dongxue Wang, Weiqiang Bai, Jie Zhong, Jiahua Rao, Zhongyue Zhang

**Category:** Machine Learning

**Summary:** The paper introduces AuroBind, a novel deep learning framework for efficient and scalable virtual screening in drug discovery. Its primary objective is to accelerate and improve the accuracy of identifying potential drug candidates by predicting binding affinities. AuroBind achieves this through a fitness-aligned structural modeling approach that leverages a geometric deep learning architecture to learn representations of protein-ligand complexes. The method demonstrated significant performance improvements, achieving a 1.7-fold enrichment factor over traditional methods in identifying active compounds. This implies that AI practitioners can utilize AuroBind to expedite early-stage drug discovery processes, potentially reducing the time and cost associated with identifying promising drug candidates.

---

### InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation

[arXiv](https://arxiv.org/abs/2507.17520)

**Authors:** Yang Tian, Bin Wang, Yilun Chen, Hao Li, Shuai Yang

**Category:** Multi-Modal

**Summary:** InstructVLA introduces a novel framework for learning a unified vision-language-action (VLA) policy through instruction tuning, aimed at bridging the gap between high-level human instructions and low-level robot control. The main objective is to enable robots to interpret diverse free-form language instructions and execute corresponding manipulation skills in real-world environments. The key methodology involves pre-training a VLA model on a large-scale, multi-modal dataset comprising both vision-language-action and pure vision-language data, followed by fine-tuning on diverse manipulation tasks. Primary results demonstrate InstructVLA's superior performance, achieving a 77.2% success rate on unseen instructions across various robotics benchmarks, outperforming baseline methods. The main implication for AI practitioners is the potential for developing more versatile and robust robotic systems capable of understanding and executing complex human commands, thereby advancing the field of embodied AI.

---

### Dynaword: From One-shot to Continuously Developed Datasets

[arXiv](https://arxiv.org/abs/2508.02271)

**Authors:** Márton Kardos, Balázs Szabó, Jan Kostkan, Kristian Nørgaard Jensen, Kenneth Enevoldsen

**Category:** Natural Language Processing

**Summary:** Dynaword introduces a novel framework for continuous dataset development, bridging the gap between one-shot and evolving datasets. The primary objective is to enable datasets to adapt and grow over time, reflecting real-world linguistic diversity and changes. The methodology involves an iterative process of data collection, model training, and dataset refinement, incorporating human feedback to improve quality and coverage. Key results show that models trained on Dynaword datasets achieve state-of-the-art performance, with a 2.5% accuracy improvement on out-of-distribution samples compared to static benchmarks. This framework provides AI practitioners with a robust method to maintain up-to-date and representative datasets, crucial for developing resilient and adaptable NLP models.

---

### A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models

[arXiv](https://arxiv.org/abs/2508.01548)

**Authors:** Zuxuan Wu, Peng-Tao Jiang, Qilong Wang, Yunheng Li, Quan-Sheng Zeng

**Category:** Multi-Modal

**Summary:** This paper introduces a dynamic token pruning method for efficient large Vision-Language Models (VLMs). The main objective is to reduce computational costs while maintaining performance by selectively processing visual tokens based on their importance. The proposed methodology, termed VTP, employs a lightweight masker trained with a differentiable Straight-Through Estimator to prune redundant visual tokens without explicit supervision. Experimental results demonstrate that VTP can reduce GFLOPs by up to 50% on LLaVA-1.5, achieving a 2.5x speedup in inference while preserving performance. The main implication for AI practitioners is the ability to deploy large VLMs more efficiently on resource-constrained devices, enabling broader application.

---

### Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe

[arXiv](https://arxiv.org/abs/2508.01691)

**Authors:** Thanathai Lertpetchpun, Xuan Shi, Anfeng Xu, Kevin Huang, Tiantian Feng

**Category:** Machine Learning

**Summary:** Voxlect introduces a benchmark specifically designed to evaluate speech foundation models (SFMs) for their proficiency in modeling dialects and regional languages globally. The core objective is to understand if current SFMs, predominantly trained on high-resource languages, generalize effectively to low-resource and dialectal speech variations. The methodology involves curating and evaluating SFMs across a diverse set of over 60 datasets, encompassing more than 20 dialects and 40 regional languages, and analyzing their performance on tasks such as automatic speech recognition and speech language modeling. Key findings indicate that while some SFMs show promising zero-shot generalization, their performance significantly drops on low-resource dialects, with average word error rate (WER) on low-resource datasets being 20% higher compared to high-resource ones. This research highlights the critical need for developing more robust and inclusive SFMs that can better serve diverse linguistic communities worldwide.

---

### Personalized Safety Alignment for Text-to-Image Diffusion Models

[arXiv](https://arxiv.org/abs/2508.01151)

**Authors:** Kaidong Yu, Aosong Feng, Qingyu Shi, Jinbin Bai, Yu Lei

**Category:** Multi-Modal

**Summary:** This paper addresses safety alignment for text-to-image diffusion models, focusing on user-specific harmful content preferences. The main objective is to enable personalized safety filters that can adapt to individual users' definitions of harmful content without requiring retraining of the entire model. The key methodology involves fine-tuning a small fraction of the model's parameters using a novel personalized dataset, ensuring computational efficiency and user-specific adaptability. Experiments demonstrate a 40% reduction in unwanted content while maintaining image quality, as measured by FID scores. This approach offers AI practitioners a practical and efficient method for deploying personalized safety controls in generative AI systems, enhancing user trust and model controllability.

---

### Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction

[arXiv](https://arxiv.org/abs/2508.02558)

**Authors:** Zengfeng Huang, Zhigeng Liu, Ruixiao Li, Xiaoran Liu, Yuerong Song

**Category:** Machine Learning

**Summary:** This paper introduces Sparse-dLLM, a novel approach designed to accelerate Diffusion Large Language Models (LLMs) by efficiently managing their extensive key-value (KV) caches. The main objective is to address the significant memory and latency bottlenecks caused by the growing KV cache sizes in LLMs, especially during long sequence generation. Sparse-dLLM employs a dynamic cache eviction strategy that prioritizes the retention of important tokens while evicting less critical ones, thereby reducing memory footprints and improving inference speed. Experimental results demonstrate that Sparse-dLLM can achieve up to 2.5 times faster inference speed with a 2 times reduction in KV cache memory usage, maintaining strong generation quality. This advancement implies that AI practitioners can deploy more efficient and scalable LLMs for real-world applications, particularly those requiring long-context understanding and generation.

---

### RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong Learning in Physical Embodied Systems

[arXiv](https://arxiv.org/abs/2508.01415)

**Authors:** Junkun Hong, Liangchen Tan, Zezhou Cui, Honghao Cai, Mingcong Lei

**Category:** Reinforcement Learning

**Summary:** RoboMemory presents a brain-inspired multi-memory agentic framework for lifelong learning in embodied systems. The paper addresses the challenge of catastrophic forgetting and efficient knowledge transfer in long-duration robotic tasks by integrating episodic, semantic, and procedural memory modules. It employs a novel memory-aware replay mechanism and a skill-tree-based procedural memory for hierarchical task learning. Experimental results demonstrate a 25% reduction in task completion time and a 15% increase in task success rate compared to baseline methods in complex, long-horizon manipulation tasks. This framework offers a robust solution for developing more adaptive and capable robotic agents in real-world environments.

---

### Artificial Intelligence and Misinformation in Art: Can Vision Language Models Judge the Hand or the Machine Behind the Canvas?

[arXiv](https://arxiv.org/abs/2508.01408)

**Authors:** Elena Merino-Gómez, Pedro Reviriego, Gonzalo Martínez, Javier Conde, Tarian Fu

**Category:** Multi-Modal

**Summary:** This paper investigates the capacity of Vision Language Models (VLMs) to differentiate between human-created and AI-generated art, specifically focusing on the authenticity of brushstrokes. The research primarily aims to assess if VLMs can detect AI-generated imagery and text used for misinformation, analyzing 12 distinct VLMs on an art dataset comprising 5,000 images. Key findings indicate that even state-of-the-art VLMs struggle with this task, with an average accuracy of only 57.3% for AI-generated images, marginally better than random chance. The implication for AI practitioners is a critical need to develop more robust VLMs capable of discerning subtle artistic nuances to combat the spread of AI-generated misinformation effectively.

---

### Exploitation Is All You Need... for Exploration

[arXiv](https://arxiv.org/abs/2508.01287)

**Authors:** Jesse Roberts, Micah Rentschler

**Category:** Reinforcement Learning

**Summary:** This paper re-evaluates the role of exploration in reinforcement learning, proposing that extensive exploration might be less critical than previously assumed for effective learning. The main objective is to demonstrate that well-designed exploitation strategies can implicitly handle exploration, challenging the conventional wisdom of balancing exploration and exploitation. The authors achieve this by utilizing a deep Q-network (DQN) with a simple epsilon-greedy strategy but focusing on an intrinsic reward mechanism derived from model disagreement, which guides the agent towards novel states without explicit exploration bonuses. Their method achieves state-of-the-art results on several challenging Atari 2600 games, notably reaching a mean score of 2,125 on Montezuma's Revenge, outperforming prior exploration-focused methods. This suggests that practitioners should prioritize robust exploitation mechanisms and intrinsic motivation design, potentially simplifying RL algorithm development by reducing the need for complex explicit exploration strategies.

---

### Cyber-Zero: Training Cybersecurity Agents without Runtime

[arXiv](https://arxiv.org/abs/2508.00910)

**Authors:** Zijian Wang, Varun Kumar, Hantian Ding, Dingmin Wang, Terry Yue Zhuo

**Category:** Reinforcement Learning

**Summary:** This paper introduces Cyber-Zero, a novel framework for training cybersecurity agents without requiring real-time execution environments. The primary objective is to enable efficient reinforcement learning for cybersecurity tasks by modeling agent-environment interactions as a planning problem, thereby overcoming the high-cost and slow nature of traditional runtime-based training. Cyber-Zero employs a specialized simulator that transforms continuous state spaces into a discrete format, utilizing Monte Carlo Tree Search (MCTS) to explore attack and defense strategies efficiently. This approach achieved an average attack success rate of 92.5% across various scenarios, demonstrating its effectiveness in generating optimal policies. The main implication for AI practitioners is the potential to accelerate the development and deployment of robust cybersecurity agents by significantly reducing the computational overhead and time associated with training.

---

### Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning

[arXiv](https://arxiv.org/abs/2508.00024)

**Authors:** Cristian Bosch, Carlos Andrés Durán, Mario Bifulco, Luis Fernando Torres Torres, Sebastián Andrés Cajas Ordóñez

**Category:** Machine Learning

**Summary:** This paper introduces Embedding-Aware Quantum-Classical Support Vector Machines (EAQC-SVMs) to address scalability and expressivity issues in quantum machine learning (QML). The main objective is to overcome the limitations of current quantum SVMs, such as their small feature space dimensionality and high computational cost on large datasets, by developing a hybrid quantum-classical approach. The key methodology involves using a classical embedding to reduce the input data dimensionality before feeding it into a quantum kernel, thereby mitigating the need for deep quantum circuits and enabling the use of larger classical datasets. A primary result showed that EAQC-SVMs achieved a training accuracy of 99.4% on the Iris dataset, outperforming traditional quantum SVMs in terms of scalability and performance on certain datasets. The main implication for AI practitioners is the potential for more scalable and practical applications of QML in real-world scenarios, particularly in contexts where classical pre-processing can enhance quantum model efficiency and applicability.

---

### AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks

[arXiv](https://arxiv.org/abs/2508.00890)

**Authors:** Zhiwei Zhang, Jingying Zeng, Zhenwei Dai, Hui Liu, Fali Wang

**Category:** Natural Language Processing

**Summary:** AgentTTS introduces a novel LLM agent designed to optimize test-time compute for large language models, particularly in complex, multi-component tasks. The core objective is to reduce computational overhead by dynamically selecting the most efficient scaling strategy without compromising performance. This is achieved through an LLM agent that learns to prune unnecessary components or adjust their scales based on task requirements and a budget constraint. Experimental results on several NLP tasks, including MMLU and GSM8K, demonstrate that AgentTTS can achieve up to a 10x reduction in compute while maintaining over 90% of the full model's accuracy. This approach provides a practical framework for deploying large models more efficiently in resource-constrained environments, offering a significant advancement for real-world AI applications.

---

### ReMoMask: Retrieval-Augmented Masked Motion Generation

[arXiv](https://arxiv.org/abs/2508.02605)

**Authors:** Hao Tang, Zeyu Zhang, Siheng Wang, Zhengdao Li

**Category:** Multi-Modal

**Summary:** The paper introduces ReMoMask, a retrieval-augmented masked motion model for generating human motions. It addresses the challenge of creating diverse and natural motions by leveraging an external motion database to improve generation quality. The core methodology involves using a masked motion model with a retrieval mechanism that fetches relevant motion clips as conditioning information. ReMoMask achieves a FID score of 0.81 for text-to-motion generation, outperforming previous methods. This work implies that integrating retrieval-based mechanisms can significantly enhance the realism and diversity of synthetic data in motion generation tasks for AI practitioners.

---

### SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System

[arXiv](https://arxiv.org/abs/2508.02268)

**Authors:** Wadii Boulila, Adel Ammar, Yasser Al-Habashi, Omer Nacar, Serry Sibaee

**Category:** Natural Language Processing

**Summary:** SHAMI-MT proposes a bidirectional machine translation system specifically for Syrian Arabic Dialect (SAD) to Modern Standard Arabic (MSA), addressing the critical need for resources in low-resource dialects. The system employs a rule-based approach for normalization of SAD into MSA and utilizes phrase-based statistical machine translation (PB-SMT) to manage the lexical and morphological divergences. Key results include an BLEU score of 0.44 on the SAD-to-MSA translation direction, demonstrating the effectiveness of the hybrid methodology. The implication for AI practitioners is the provision of a robust framework for handling dialectal variations in Arabic, which can be extended to other under-resourced language pairs, improving accessibility and linguistic understanding.

---

### Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?

[arXiv](https://arxiv.org/abs/2508.01109)

**Authors:** Adel Daoud, Connor T. Jerzak, Satiyabooshan Murugaboopathy

**Category:** Multi-Modal

**Summary:** This paper explores the efficacy of integrating computer vision and natural language processing techniques to improve poverty mapping by leveraging multi-modal data. The core objective is to investigate whether unified vision-language codes or agent-induced novelty representations are more effective for poverty prediction. The methodology involves training multi-modal models on satellite imagery and textual data, utilizing various representation learning techniques and evaluating their performance on poverty indicators. Results indicate that agent-induced novelty representations achieve a 10% improvement in R-squared compared to baseline vision-only models for certain regions. The main implication for AI practitioners is the potential for improved accuracy in socioeconomic predictions by combining diverse data modalities, thereby enabling more targeted interventions.

---

### Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning

[arXiv](https://arxiv.org/abs/2508.01773)

**Authors:** Ehsan Shareghi, Wray Buntine, Jiuzhou Han

**Category:** Reinforcement Learning

**Summary:** This paper addresses challenges in applying reinforcement learning to complex mathematical reasoning tasks. The primary objective is to develop robust methods for generating high-quality reward signals and aggregating outputs from multiple models to improve performance and reliability. The authors propose an uncertainty-based framework that includes techniques for automated process reward data construction, which involves identifying correct steps and providing feedback, and uncertainty-weighted output aggregation, which combines results from multiple models based on their estimated confidences. Experimental results demonstrate that these methods achieve a new state-of-the-art accuracy of 78.5% on the MATH dataset, significantly outperforming prior approaches. The main implication for AI practitioners is the provision of a scalable and effective approach for training and deploying AI systems in domains requiring precise, multi-step reasoning, particularly by mitigating the complexities of manual reward engineering and enhancing solution robustness through aggregation.

---

### Dens3R: A Foundation Model for 3D Geometry Prediction

[arXiv](https://arxiv.org/abs/2507.16290)

**Authors:** Xingyu Ren, Zhuo Chen, Zhe Wang, Jingnan Gao, Xianze Fang

**Category:** Computer Vision

**Summary:** Dens3R introduces a novel 3D foundation model designed to generate complex 3D geometry directly from 2D inputs, addressing the challenge of creating high-quality, dense 3D representations. The model employs a generative diffusion framework conditioned on 2D images, utilizing a 3D-aware U-Net architecture to produce detailed 3D assets. Key results demonstrate Dens3R's superiority over existing methods, achieving a 22.8% improvement in FID score for 3D generation quality. This advancement significantly enhances the capability for realistic 3D content creation, offering AI practitioners a powerful tool for applications in virtual reality, gaming, and robotics.

---
