# daily-papers

## 2025-08-11


### On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification

[arXiv](https://arxiv.org/abs/2508.05629)

**Authors:** Xinyu Ye, Yingzhe Peng, Zhou Ziheng, Yizhou Zhou, Yongliang Wu

**Category:** Reinforcement Learning

**Summary:** This paper investigates the generalization capabilities of Supervised Fine-Tuning (SFT) from a Reinforcement Learning (RL) perspective, specifically focusing on how reward discrepancies affect performance. The main objective is to understand why SFT often outperforms Proximal Policy Optimization (PPO) in out-of-distribution generalization and to propose a method for improving SFT. The authors propose a novel reward rectification technique that refines the reward model by reducing miscalibration and noise, effectively bridging the gap between SFT and PPO. Their experiments show that the proposed reward rectification significantly improves SFT's out-of-distribution performance, achieving up to a 6.2% increase in success rate on certain tasks. This implies that carefully engineered reward models, even when applied to SFT, can lead to substantial improvements in model generalization, offering a more stable alternative to complex RL methods like PPO.

---

### R-Zero: Self-Evolving Reasoning LLM from Zero Data

[arXiv](https://arxiv.org/abs/2508.05004)

**Authors:** Zongxia Li, Hongming Zhang, Xiaoyang Wang, Wenhao Yu, ChengsongHuang

**Category:** Natural Language Processing

**Summary:** This paper introduces R-Zero, a novel framework for training reasoning-capable Large Language Models (LLMs) from scratch without relying on existing reasoning data. The primary objective is to enable LLMs to develop advanced reasoning abilities autonomously through an iterative self-improvement process. R-Zero employs a self-evolutionary mechanism where an LLM generates its own reasoning data, evaluates it, and then refines its capabilities based on this self-generated feedback, akin to a curriculum learning approach. Experimental results demonstrate that R-Zero significantly enhances reasoning performance, achieving up to a 10% improvement on complex reasoning benchmarks compared to models trained with traditional supervised methods. This approach implies that AI practitioners can now develop highly capable reasoning LLMs with reduced dependence on large, manually curated reasoning datasets, potentially accelerating research and deployment in critical reasoning-intensive applications.

---

### Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation

[arXiv](https://arxiv.org/abs/2508.05635)

**Authors:** Siyuan Huang, Pengfei Zhou, Yue Liao, sundrops, jianlanluo

**Category:** Reinforcement Learning

**Summary:** This paper introduces Genie Envisioner, a unified world foundation platform designed to enable robotic manipulation through advanced AI. The primary objective is to create a general-purpose, goal-conditioned manipulation model capable of zero-shot transfer across diverse robotic embodiments and tasks. The methodology involves leveraging a large-scale, multi-modal dataset from Internet videos and pre-training a transformer-based world model on pixel observations and actions, followed by finetuning with high-quality robot data and incorporating goal conditioning. Genie Envisioner demonstrates significant improvements, achieving a 77.4% success rate on real-world manipulation tasks, outperforming prior methods by over 10%. The main implication for AI practitioners is the potential for developing highly versatile and transferable robotic manipulation systems using large-scale pre-training and unified world models, reducing the need for extensive task-specific data.

---

### DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning

[arXiv](https://arxiv.org/abs/2508.05405)

**Authors:** Ziming Wang, Ye Wang, Pi Bu, Xinrun Xu, tellarin

**Category:** Multi-Modal

**Summary:** DeepPHY is a new benchmark that assesses the physical reasoning capabilities of agentic Vision-Language Models (VLMs). The paper's primary objective is to evaluate how well current VLMs can perform tasks requiring an understanding of physical principles, such as stability and collision. It introduces a challenging dataset with over 1500 video-based questions requiring complex physical understanding, designed to be out-of-distribution from common training sets, and proposes an agentic framework for testing. Results indicate that even leading models like GPT-4o achieve a mean accuracy of only 34.6%, highlighting significant deficiencies in current VLMs' physical reasoning. This research implies that AI practitioners must focus on developing new architectures and training methodologies that explicitly imbue VLMs with robust physical common sense and reasoning abilities to improve their real-world applicability.

---

### Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity

[arXiv](https://arxiv.org/abs/2508.05609)

**Authors:** Zhibing Li, Tong Wu, Ziyang Chu, Long Zhuo, ZhangYuhan

**Category:** Computer Vision

**Summary:** Hi3DEval introduces a novel hierarchical evaluation framework for 3D generation, addressing the limitations of existing metrics that often overlook fidelity, diversity, and usability. The primary objective is to develop a comprehensive metric suite that better aligns with human perception by assessing both fine-grained local details and overall global structure. The methodology involves a multi-level approach, incorporating 2D-based and 3D-based metrics to evaluate various aspects like geometric validity, texture quality, and semantic consistency. Quantitative results demonstrate that Hi3DEval achieves a higher correlation with human perceptual judgments, with a reported Pearson correlation coefficient of 0.85, significantly outperforming traditional metrics. This framework provides AI practitioners with a more robust and reliable tool for assessing and improving 3D generative models, guiding the development of more realistic and usable 3D content.

---

### Are Today's LLMs Ready to Explain Well-Being Concepts?

[arXiv](https://arxiv.org/abs/2508.03990)

**Authors:** Huan Liu, Chengshuai Zhao, Zhen Tan, Bohan Jiang, BillAvan

**Category:** Natural Language Processing

**Summary:** This paper investigates the proficiency of large language models (LLMs) in explaining well-being concepts, comparing their performance to human benchmarks. The primary objective was to assess whether current LLMs can provide accurate, comprehensive, and nuanced explanations of complex psychological constructs related to well-being. The methodology involved prompting various LLMs (GPT-3.5, GPT-4, LLaMA2-Chat) to explain well-being concepts, with human experts then evaluating these explanations across multiple criteria such as accuracy, understandability, and completeness. Results indicate a significant gap, with LLM explanations receiving an average quality score of 2.95 out of 5, considerably lower than human-generated explanations which scored 4.2 out of 5. The main implication for AI practitioners is the necessity for further research into fine-tuning LLMs with domain-specific knowledge and developing more robust evaluation metrics for conceptual understanding beyond mere factual recall.

---

### Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?

[arXiv](https://arxiv.org/abs/2508.03644)

**Authors:** Junjie Yang, Dongping Chen, Yaochen Wang, Mingjia Wang, Wenxuan Shen

**Category:** Natural Language Processing

**Summary:** This paper critically evaluates current assessment methodologies for Document Retrieval-Augmented Generation (RAG) systems. The primary objective is to investigate whether existing evaluation paradigms adequately capture the true performance and limitations of RAG models. The study employs a multi-faceted approach, including analyzing popular benchmarks like ANLI and FEVER, and proposing new metrics such as RAG-Correctness and RAG-Coverage to specifically target the retrieval and generation components. Results indicate that current benchmarks may overstate RAG performance, with the proposed RAG-Correctness metric showing a significant discrepancy of up to 25% lower scores compared to standard methods. The main implication for AI practitioners is the need for more nuanced and comprehensive evaluation frameworks to accurately assess and improve RAG systems beyond simplistic accuracy metrics.

---

### CoAct-1: Computer-using Agents with Coding as Actions

[arXiv](https://arxiv.org/abs/2508.03923)

**Authors:** Taiwei Shi, Jieyu Zhang, Viraj Prabhu, Yutong Dai, linxinso

**Category:** Reinforcement Learning

**Summary:** CoAct-1 introduces a novel framework for creating computer-using agents that leverage coding as a primary action for task completion. The main objective is to overcome limitations of previous agents by enabling more complex and flexible interaction with computing environments through programmatic control. The methodology involves a multi-modal large language model (LLM) and a tree-of-thought (ToT) approach, allowing the agent to generate and execute code, observe results, and self-correct. In evaluations, CoAct-1 demonstrated a 15% absolute improvement over ReAct on the MiniWob++ dataset, showcasing enhanced robustness and adaptability. This implies that AI practitioners can develop more capable and generalizable agents for complex computational tasks by integrating code-centric action spaces.

---

### Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability

[arXiv](https://arxiv.org/abs/2508.04017)

**Authors:** Yuan Wu, Yi Chang, Gengxu Li, Jinzhe Li, Haiqi Yang

**Category:** Multi-Modal

**Summary:** This paper introduces a systematic framework to evaluate the input scrutiny ability of Large Multimodal Models (LMMs) in recognizing faulty inputs. The research aims to understand if LMMs can actively identify and acknowledge errors in their multimodal inputs, such as conflicting image-text information or out-of-distribution content. The methodology involves creating 12 distinct types of faulty inputs across seven datasets, evaluating 14 LMMs on their ability to detect these faults, and analyzing their responses. Key findings reveal that LMMs struggle significantly with input scrutiny, with top-performing models achieving only about 50% accuracy on faulty input detection and often hallucinating when encountering such inputs. This implies that AI practitioners must implement robust pre-processing and input validation mechanisms, or finetune LMMs specifically for fault detection, to ensure reliability and prevent models from generating erroneous outputs based on flawed inputs.

---

### Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models

[arXiv](https://arxiv.org/abs/2508.02120)

**Authors:** Fangzhou Yao, Weibo Gao, Yizhi Wang, Linan Yue, yichaodu

**Category:** Natural Language Processing

**Summary:** This survey provides an extensive review of efficient R1-style Large Reasoning Models, specifically focusing on methods that optimize performance without sacrificing reasoning capabilities. The main objective is to categorize and analyze diverse architectural innovations, training strategies, and inference optimizations that enhance efficiency in large language models for complex reasoning tasks. Key methodologies discussed include quantization techniques like QAT and PTQ, architectural modifications such as mixture-of-experts (MoE) and sparse attention mechanisms, and distillation for model compression. Results demonstrate that models leveraging these efficiencies can achieve up to a 70% reduction in computational cost while maintaining competitive performance on benchmarks like GSM8K. The main implication for AI practitioners is the availability of validated strategies to deploy powerful reasoning models in resource-constrained environments, enabling broader applications of advanced AI.

---

### Marco-Voice Technical Report

[arXiv](https://arxiv.org/abs/2508.02038)

**Authors:** Qingjuan Li, Haoqin Sun, Chenyang Lyu, Fengping Tian, Patrick828

**Category:** Multi-Modal

**Summary:** Marco-Voice introduces a novel, efficient framework for multi-modal large language models (LLMs) that integrates visual and audio inputs with text. The core objective is to achieve highly performant and generalized multi-modal understanding and generation, particularly focusing on robust audio-visual learning. The methodology involves a unified architecture that processes diverse modalities into a shared representation space, leveraging an innovative tokenization and alignment strategy for cross-modal fusion. Key results include achieving a 23.9% relative performance gain on the LVLM-eHub benchmark for audio-visual tasks, demonstrating superior efficiency and scalability compared to prior multi-modal models. The main implication for AI practitioners is the provision of a robust and efficient framework for developing general-purpose multi-modal AI systems capable of handling complex real-world scenarios requiring integrated sensory understanding.

---

### Evaluating, Synthesizing, and Enhancing for Customer Support Conversation

[arXiv](https://arxiv.org/abs/2508.04423)

**Authors:** Feng Chen, Lifan Guo, Junhui Li, Huaixia Dou, amazingj

**Category:** Natural Language Processing

**Summary:** This paper focuses on improving AI's ability to evaluate, synthesize, and enhance customer support conversations, crucial for real-world applications. The core objective is to overcome limitations of traditional NLP methods in handling the unstructured and complex nature of customer support dialogues. The methodology involves a multi-pronged approach: developing new evaluation metrics tailored to conversation quality, proposing synthesis techniques for generating more realistic conversation data, and implementing enhancement strategies like intent recognition and response generation. Key results indicate that the proposed methods lead to a 15% increase in F1-score for intent recognition and a noticeable improvement in dialogue coherence as judged by human evaluators. The main implication for AI practitioners is the provision of robust tools and methodologies for building more effective and empathetic AI-powered customer support systems, reducing the gap between academic research and practical deployment.

---

### InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities

[arXiv](https://arxiv.org/abs/2508.05496)

**Authors:** Zhijie Sang, Kejing Yang, Qi Zhou, Su Lu, Shuo Cai

**Category:** Natural Language Processing

**Summary:** This paper presents InfiAlign, a novel framework designed to improve the reasoning abilities of large language models (LLMs) through scalable and sample-efficient alignment. The main objective is to overcome the limitations of current alignment methods, such as their high computational cost and data inefficiency, which hinder the development of advanced reasoning in LLMs. InfiAlign achieves this by integrating an auxiliary reward model that infers fine-grained reasoning skills, along with a novel policy update rule that utilizes these inferred skills as soft labels for policy learning. Experimental results demonstrate significant improvements, with InfiAlign achieving a 45% win rate against GPT-4 on the Big Bench Hard benchmark and outperforming other methods like DPO, IPO, and PPO by an average of 15% on reasoning tasks. The framework offers a practical solution for AI practitioners to enhance the reasoning capabilities of LLMs more efficiently, potentially leading to the deployment of more robust and intelligent AI systems in various applications.

---

### MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes

[arXiv](https://arxiv.org/abs/2508.05630)

**Authors:** Xudong Jiang, Shuting He, Chang Liu, Kaining Ying, Henghui Ding

**Category:** Computer Vision

**Summary:** This paper introduces MOSEv2, a novel and more challenging dataset designed to advance research in video object segmentation (VOS) within complex real-world environments. The primary objective is to address the limitations of existing VOS datasets, which often lack diversity and struggle with occlusion, small objects, and intricate backgrounds. MOSEv2 achieves this by incorporating 500 high-resolution video clips with diverse categories and fine-grained annotations of multiple instances per video. Experimental results demonstrate that state-of-the-art VOS models, such as XMem and IDOL, experience a significant performance drop of 10-20% when evaluated on MOSEv2 compared to established benchmarks like MOSE, highlighting the dataset's increased complexity and efficacy in revealing model weaknesses. MOSEv2 provides a critical benchmark for developing more robust and generalizable VOS models capable of real-world deployment by pushing the boundaries of current techniques.

---

### StrandDesigner: Towards Practical Strand Generation with Sketch Guidance

[arXiv](https://arxiv.org/abs/2508.01650)

**Authors:** Xiaobin Hu, Han Feng, Chengming Xu, Moran Li, Na Zhang

**Category:** Computer Vision

**Summary:** The paper \"StrandDesigner: Towards Practical Strand Generation with Sketch Guidance\" addresses the challenge of efficiently generating realistic 3D hair strands. The core objective is to overcome the limitations of traditional hair modeling by enabling practical and intuitive strand generation through sketch-guided techniques. The key methodology involves a novel differentiable hair rendering pipeline combined with an inverse problem approach that reconstructs 3D strands from 2D sketches. Their method significantly reduces the number of required strokes, achieving plausible results with as few as 2-3 strokes per hair region, and demonstrates high fidelity in shape and style reproduction compared to existing techniques. This research implies a substantial improvement for AI practitioners in computer graphics and animation, enabling more efficient and artist-friendly hair modeling workflows.

---

### Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression

[arXiv](https://arxiv.org/abs/2508.04979)

**Authors:** Yifei Ji, Jiale Yuan, Jinpei Guo, Mingde Zhou, Zheng Chen

**Category:** Computer Vision

**Summary:** This paper presents a novel approach to image compression using a steered one-step diffusion model, addressing the critical need for efficient and high-quality image data handling. The main objective is to overcome the limitations of traditional diffusion-based compression methods, specifically their slow inference speeds and high computational costs. The authors propose integrating a fidelity-rich decoder within the diffusion process, which steers the one-step model to generate high-quality compressed images by optimizing the trade-off between reconstruction accuracy and bit rate. A key result demonstrates that their method achieves a 0.2-0.5 dB PSNR gain over existing state-of-the-art diffusion-based compressors while reducing inference time by 80-90%. This research implies that AI practitioners can now leverage diffusion models for practical, real-time image compression applications without sacrificing quality, opening new avenues for efficient media processing and storage.

---

### Learning to Reason for Factuality

[arXiv](https://arxiv.org/abs/2508.05618)

**Authors:** Rulin Shao, Barlas Oğuz, Vincent-Pierre Berges, Ilia Kulikov, Xilun Chen

**Category:** Natural Language Processing

**Summary:** The paper "Learning to Reason for Factuality" addresses the critical challenge of factual consistency in text generation, particularly within large language models (LLMs). Its primary objective is to develop a method for LLMs to reason about the factual correctness of their generated content and revise it if necessary, rather than simply identifying errors. The authors propose an iterative reasoning-revision framework employing a specialized Reasoner-Reviser model, which significantly outperforms standard LLM generation and self-correction methods. The approach achieves a 22.8% improvement in factuality on the FActEval benchmark compared to baseline GPT-4 models without external tools. This research implies that AI practitioners should focus on building explicit reasoning and revision modules into their LLM-powered applications to enhance factual accuracy and trustworthiness.

---

### Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling

[arXiv](https://arxiv.org/abs/2508.03404)

**Authors:** Ruolin Shen, Shilin Lu, Yudong Zhang, Zhangquan Chen, Xinlei Yu

**Category:** Multi-Modal

**Summary:** This paper presents a novel multi-agent collaboration framework for Visual Document Understanding and Question Answering. The main objective is to enhance the performance and robustness of VDU and QA systems by leveraging a collaborative multi-agent architecture. The key methodology involves a 'test-time scaling' strategy, dynamically adjusting the number of agents to optimize resource utilization and accuracy. Primary results show that the framework significantly outperforms single-agent models, achieving a 2.5% improvement in F1 score on complex VDU tasks. This implies that AI practitioners can achieve more efficient and robust VDU and QA systems by adopting adaptive multi-agent frameworks.

---

### I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking

[arXiv](https://arxiv.org/abs/2508.02243)

**Authors:** Chao Wang, Tong Ruan, Kaiwen Li, Junwen Li, Ziyan Liu

**Category:** Multi-Modal

**Summary:** This paper introduces I2CR, a novel framework designed to enhance multimodal entity linking by improving cross-modal alignment. The core objective is to address the limitations of existing methods that often struggle with accurately linking entities across different modalities, specifically text and images. I2CR employs a collaborative reflection mechanism, which integrates intra-modal and inter-modal interactions to refine entity representations and strengthen their connections across modalities. Experiments demonstrate that I2CR significantly outperforms state-of-the-art methods, achieving a 2.1% F1-score improvement on the WikiMEL dataset. This advancement implies that AI practitioners can leverage I2CR to build more robust and accurate multimodal understanding systems, particularly in applications requiring precise entity linking across diverse data types.

---

### PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction

[arXiv](https://arxiv.org/abs/2508.05545)

**Authors:** Lavanya Elluri, Aritran Piplai, Anantaa Kotal, Leon Garza, amanchadha

**Category:** Natural Language Processing

**Summary:** This paper assesses Large Language Models (LLMs) for Personally Identifiable Information (PII) redaction. The research aims to quantify the capabilities and risks of LLMs in redacting PII from unstructured text, focusing on the trade-off between recall (redaction of PII) and precision (minimizing false positives). The methodology involves evaluating various LLMs using a newly constructed benchmark dataset, PRvL, which includes diverse PII types and contexts. Key results indicate that even state-of-the-art LLMs struggle with high recall, achieving only around 70% recall on average, and often exhibit significant false positive rates. The main implication for AI practitioners is the need for careful consideration and potentially additional fine-tuning or hybrid approaches when deploying LLMs for sensitive PII redaction tasks, given their current limitations in accuracy and reliability.

---

### REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation

[arXiv](https://arxiv.org/abs/2508.04946)

**Authors:** Xiao Yu, Mahesh Kumar Nandwana, Joseph Liu, Nameer Hirschkind

**Category:** Natural Language Processing

**Summary:** This paper introduces REINA, a novel regularized entropy information-based loss function designed to improve the efficiency and quality of simultaneous speech translation. The main objective is to address the trade-off between translation latency and quality by leveraging information theory principles to guide the model's decision-making process. REINA's methodology involves dynamically adjusting the waiting policy based on the entropy of information flow, encouraging the model to translate only when sufficient information is available and penalizing early, less informed predictions. Experiments demonstrate that REINA significantly improves translation quality, achieving an average BLEU score increase of 1.5 compared to baselines, while maintaining competitive latency. The implication for AI practitioners is the provision of a more robust and adaptable training paradigm for real-time speech translation systems, potentially enabling better user experiences in live communication scenarios.

---

### I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations

[arXiv](https://arxiv.org/abs/2508.04939)

**Authors:** Chirag Shah, Tanya Roosta, Julia Kharchenko, amanchadha

**Category:** Natural Language Processing

**Summary:** This paper investigates the potential for Large Language Models (LLMs) to detect linguistic shibboleths in hiring evaluations, a phenomenon where specific language use patterns might implicitly disqualify candidates. The main objective is to establish a benchmark for evaluating LLMs' ability to identify these subtle linguistic cues, focusing on professional communication scenarios. The methodology involves creating a dataset of text passages with varying degrees of linguistic shibboleths and evaluating several LLMs (including GPT-3.5 and Llama2) on their classification accuracy. Preliminary results indicate that state-of-the-art LLMs struggle with this task, achieving an average F1-score of only 0.45, highlighting their current limitations in nuanced sociolinguistic understanding. The implication for AI practitioners is that LLMs are not yet reliable for sensitive tasks like screening for linguistic shibboleths in high-stakes environments such as hiring, necessitating further research into more robust and ethically aligned models.

---

### Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis

[arXiv](https://arxiv.org/abs/2508.04699)

**Authors:** Yashwanth Babu, Srujana Pillarichety, Isha Nalawade, Anushka Yadav, reshmighosh

**Category:** Natural Language Processing

**Summary:** This paper investigates the diagnostic reasons behind reasoning failures in multi-hop question answering models. The research aims to understand why large language models (LLMs) struggle with complex multi-hop analysis despite their general reasoning capabilities. The authors introduce a novel diagnostic framework and a dataset of 34K error cases to pinpoint specific failure modes. They found that models like GPT-4 often fail due to issues in information extraction (44.6%) and synthesis (35.2%), rather than simple factual errors. The findings suggest that future research should focus on improving LLMs' ability to accurately extract and synthesize information across multiple textual sources.

---

### RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation

[arXiv](https://arxiv.org/abs/2508.04190)

**Authors:** Jian Yang, Yixuan Ding, Tianfang Zhang, Yimian Dai, fengyiwu

**Category:** Computer Vision

**Summary:** RPCANet++ introduces a novel deep interpretable robust PCA network for sparse object segmentation, aiming to overcome the limitations of traditional Robust PCA (RPCA) and deep learning methods in handling real-world noise and ensuring interpretability. The paper addresses the challenge of robustly segmenting sparse objects in complex scenes, particularly in medical imaging, by integrating a deep network with an interpretable RPCA framework. Its key methodology involves a learnable optimization framework that unrolls iterations of an Augmented Lagrangian Multiplier (ALM) algorithm for RPCA, enhancing robustness against various noise types. Experimental results demonstrate that RPCANet++ achieves superior segmentation performance, exemplified by its 92.5% Dice score on polyp segmentation, outperforming state-of-the-art methods. This advancement provides AI practitioners with a robust and interpretable tool for accurate sparse object segmentation in challenging noisy environments.

---

### Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decode

[arXiv](https://arxiv.org/abs/2508.04107)

**Authors:** Hong Wang, Yefeng Zheng, Dingjiang Huang, Zhijian Wu, jcwang0602

**Category:** Multi-Modal

**Summary:** This paper investigates how to improve Referring Expression Segmentation (RES) performance by enhancing Mask-aware score predictions in Multi-modal Large Language Models (MLLMs) with a light-weight mask decode. The primary objective is to overcome the limitations of current MLLMs in RES, specifically their suboptimal Mask-aware score predictions, without necessitating large-scale retraining. The key methodology involves introducing a novel Light-weight Mask Decode (LMD) module and a Mask-guided Referring Attention (MRA) module, which are trainable components that augment MLLMs for RES. The primary results demonstrate that the proposed LMD and MRA modules significantly improve performance, achieving a 7.7% absolute gain over baseline MLLMs on RefCOCO datasets without requiring complex fine-tuning. The main implication for AI practitioners is the provision of an efficient and effective method to adapt pre-trained MLLMs for RES, enabling their application in real-world scenarios with minimal computational overhead.

---

### Attention Basin: Why Contextual Position Matters in Large Language Models

[arXiv](https://arxiv.org/abs/2508.05128)

**Authors:** Zhe Xu, Haohao Luo, Zhenqing Ling, Delong Zeng, Zihao Yi

**Category:** Natural Language Processing

**Summary:** This paper investigates the impact of input position on the performance of Large Language Models (LLMs), introducing the concept of "Attention Basin" where models prioritize tokens in specific contextual positions, typically at the beginning and end of sequences. The research aims to understand why LLMs exhibit non-uniform attention patterns across different input positions, which is critical for optimizing their performance. The methodology involves analyzing attention distributions and performance degradation on various tasks as information is moved within the context window, utilizing metrics like accuracy on retrieval tasks. Key findings indicate a significant drop in performance (e.g., up to 20% on certain tasks) when crucial information is placed in the middle of the context, confirming the presence of an attention basin. This implies that AI practitioners should carefully consider token placement within input sequences to maximize LLM performance and reliability, particularly by avoiding central placement of critical information.

---
